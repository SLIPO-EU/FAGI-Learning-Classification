{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification for POIs fusion action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from sklearn import svm \n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "feat = pd.read_csv(\"features_export.csv\")\n",
    "\n",
    "feat = feat[feat.columns.values[range(feat.shape[1]-1)]] \n",
    "X1=feat[feat[' acceptance']]\n",
    "\n",
    "X2=swap_symetric(X1)\n",
    "X= pd.concat([X1, X2], axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.pop(' acceptance')\n",
    "y1= X.pop(' nameFusionAction')\n",
    "y= pd.get_dummies(y1)\n",
    "X = transform(X, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for the creation of the NN using keras\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=97, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification process\n",
    "names1 = [\"keras\",\"keras2\", \"Random Forest\", \"Neural Net\", \n",
    "         \"DecisionTreeClassifier\"]\n",
    "\n",
    "classifiers1 = [\n",
    "    KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=15, verbose=0),\n",
    "    KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=10, verbose=0),\n",
    "    RandomForestClassifier(max_depth=9, n_estimators=3, max_features=3),\n",
    "    MLPClassifier(alpha=1.11, solver='lbfgs',max_iter=1000),\n",
    "    DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0),\n",
    "    ]\n",
    "\n",
    "names2 = [ \"Random Forest(labeled)\", \"Lnear SVM(labeld targets)\", \n",
    "         \"SVM OVO(labeld targets)\"]\n",
    "\n",
    "classifiers2 = [\n",
    "    RandomForestClassifier(max_depth=2, n_estimators=7, max_features=6),\n",
    "    svm.LinearSVC(multi_class='crammer_singer'),\n",
    "    svm.SVC(decision_function_shape='ovo'),\n",
    "    ]\n",
    "\n",
    "scr={}\n",
    "inf={}\n",
    "df3= pd.DataFrame()\n",
    "error=pd.DataFrame()\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "for name, clf in zip(names1, classifiers1):\n",
    "    scores = cross_validate(clf, X, y, cv=cv, return_train_score=True) # this function does cross validation\n",
    " \n",
    "    inf[name]= clf.get_params()\n",
    "    scr[name]=scores\n",
    "    df3[name] = [scores.get('test_score').mean(),  scores.get('train_score').mean() ]\n",
    "    error[name]  = [scores.get('test_score').std() * 2, scores.get('train_score').std() * 2]\n",
    "    print(name, \":\" ,\"Test Accuracy: %0.2f (+/- %0.2f), Train Accuracy: %0.2f (+/- %0.2f)| %2.2f sec\" % (scores.get('test_score').mean(), scores.get('test_score').std() * 2,\n",
    "                                                                                                         scores.get('train_score').mean(), scores.get('train_score').std() * 2, scores.get('fit_time').mean()) ) # print the average and variance of cv\n",
    "\n",
    "for name, clf in zip(names2, classifiers2):\n",
    "    scores = cross_validate(clf, X, y1, cv=cv, return_train_score=True) # this function does cross validation\n",
    "\n",
    "    inf[name]= clf.get_params()\n",
    "    scr[name]=scores\n",
    "    df3[name] = [scores.get('test_score').mean(),  scores.get('train_score').mean() ]\n",
    "    error[name]  = [scores.get('test_score').std() * 2, scores.get('train_score').std() * 2]\n",
    "    print(name, \":\" ,\"Test Accuracy: %0.2f (+/- %0.2f), Train Accuracy: %0.2f (+/- %0.2f)| %2.2f sec\" \n",
    "      % (scores.get('test_score').mean(), scores.get('test_score').std() * 2,\n",
    "         scores.get('train_score').mean(), scores.get('train_score').std() * 2, scores.get('fit_time').mean()) ) # print the average and variance of cv                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticklabels(names1+names2)\n",
    "\n",
    "ax = df3.rename(index={0:'test',1: 'train'}).T.plot.bar( yerr=error.rename(index={0:'test',1: 'train'}).T, ax=ax, tick_label= names)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('test2png.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a grid search for a classification aglgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(RandomForestClassifier( ),\n",
    "                  param_grid={'n_estimators': range(1,20), 'max_depth': range(1, 10),'max_features' :range(1,10)},\n",
    "                   cv=5, refit=True, n_jobs=4)\n",
    "gs.fit(X, y1)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 13))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize=16)\n",
    "\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(1, 19)\n",
    "ax.set_ylim(0.0, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_n_estimators'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = results['mean_%s_%s' % (sample, \"score\")]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, \"score\")]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % \"score\"] == 1)[0][0]\n",
    "    best_score = results['mean_test_%s' % \"score\"][best_index]\n",
    "    \n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
